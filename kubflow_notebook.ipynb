{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slgjeYgd6pWp"
   },
   "source": [
    "[![visitor][visitor-badge]][visitor-stats]\n",
    "[![ko-fi][ko-fi-badge]][ko-fi-link]\n",
    "\n",
    "# **Kohya LoRA Trainer XL**\n",
    "A Colab Notebook For SDXL LoRA Training (Fine-tuning Method)\n",
    "\n",
    "[visitor-badge]: https://api.visitorbadge.io/api/visitors?path=Kohya%20LoRA%20Trainer%20XL&label=Visitors&labelColor=%2334495E&countColor=%231ABC9C&style=flat&labelStyle=none\n",
    "[visitor-stats]: https://visitorbadge.io/status?path=Kohya%20LoRA%20Trainer%20XL\n",
    "[ko-fi-badge]: https://img.shields.io/badge/Support%20me%20on%20Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat\n",
    "[ko-fi-link]: https://ko-fi.com/linaqruf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MxF9feWshAp"
   },
   "source": [
    "| Notebook Name | Description | Link |\n",
    "| --- | --- | --- |\n",
    "| [Kohya LoRA Trainer XL](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-trainer-XL.ipynb) | LoRA Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=flat)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-trainer-XL.ipynb) |\n",
    "| [Kohya Trainer XL](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-trainer-XL.ipynb) | Native Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=flat)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-trainer-XL.ipynb) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIbwGFkJ0nTx"
   },
   "source": [
    "<hr>\n",
    "<h4><font color=\"#4a90e2\"><b>NEWS:</b></font> <i>Colab's free-tier users can now train SDXL LoRA using the diffusers format instead of checkpoint as a pretrained model.</i></h4>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTVqCAgSmie4"
   },
   "source": [
    "# **I. Prepare Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YL6YUDrJ4798"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip uninstall xformers -y\n",
    "!pip uninstall torch torchvision -y\n",
    "!pip uninstall nvidia-cublas-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-runtime-cu12 nvidia-cudnn-cu12 nvidia-cufft-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 -y\n",
    "!pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 torchtext==0.17.0 xformers nvidia-cublas-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-runtime-cu12 nvidia-cudnn-cu12 nvidia-cufft-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 --no-cache-dir\n",
    "!pip uninstall jax -y\n",
    "!pip install jax[cuda12_pip]==0.4.23 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import time\n",
    "import requests\n",
    "import torch\n",
    "from subprocess import getoutput\n",
    "from IPython.utils import capture\n",
    "\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "from urllib.parse import urlparse, unquote\n",
    "from pathlib import Path\n",
    "from huggingface_hub import HfFileSystem\n",
    "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import concurrent.futures\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XO6bFVXFALC"
   },
   "outputs": [],
   "source": [
    "!pip install nvidia-cublas-cu12==12.1.3.1\n",
    "!pip install nvidia-cuda-cupti-cu12==12.1.105\n",
    "!pip install nvidia-cuda-runtime-cu12==12.1.105\n",
    "!pip install nvidia-cudnn-cu12==8.9.2.26\n",
    "!pip install nvidia-cufft-cu12==11.0.2.54\n",
    "!pip install nvidia-cusolver-cu12==11.4.5.107\n",
    "!pip install nvidia-cusparse-cu12==12.1.0.106\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4_2K4wT3zMp_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DataFrame:  40%|████      | 2/5 [00:00<00:00, 3606.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: John, Age: 25\n",
      "Name: Emily, Age: 28\n",
      "Starting the main process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up directories: 100%|██████████| 1/1 [00:00<00:00, 1438.38it/s]\n",
      "Installing dependencies: 100%|██████████| 3/3 [00:07<00:00,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment is being prepared...\n",
      "Main process completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {'Name': ['John', 'Emily', 'Charlie', 'Sarah', 'Michael'],\n",
    "        'Age': [25, 28, 32, 29, 35]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Loop through the DataFrame and print each row with a progress bar\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing DataFrame\"):\n",
    "    if index == 2:\n",
    "        break\n",
    "    print(f\"Name: {row['Name']}, Age: {row['Age']}\")\n",
    "\n",
    "# Environment directory paths setup for Kubeflow\n",
    "root_dir = \"/home/jovyan\"\n",
    "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
    "\n",
    "repo_dict = {\n",
    "    \"qaneel/kohya-trainer\": \"https://github.com/qaneel/kohya-trainer\",\n",
    "    \"kohya-ss/sd-scripts\": \"https://github.com/kohya-ss/sd-scripts\",\n",
    "}\n",
    "\n",
    "repository = \"qaneel/kohya-trainer\"\n",
    "repo_url = repo_dict[repository]\n",
    "branch = \"main\"\n",
    "\n",
    "def clone_repo(url, dir, branch):\n",
    "    if not os.path.exists(dir):\n",
    "        os.system(f\"git clone -b {branch} {url} {dir}\")\n",
    "\n",
    "def setup_directories():\n",
    "    dirs = [repo_dir]\n",
    "    for dir in tqdm(dirs, desc=\"Setting up directories\"):\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "def install_dependencies():\n",
    "    os.system(\"pip install tqdm\")\n",
    "    requirements = [\"requests\", \"numpy\", \"pandas\"]  # Example requirements\n",
    "    for req in tqdm(requirements, desc=\"Installing dependencies\"):\n",
    "        os.system(f\"pip install {req}\")\n",
    "\n",
    "def prepare_environment():\n",
    "    # This function prepares the environment\n",
    "    print(\"Environment is being prepared...\")\n",
    "\n",
    "def main():\n",
    "    tqdm.write(\"Starting the main process...\")\n",
    "    clone_repo(repo_url, repo_dir, branch)\n",
    "    setup_directories()\n",
    "    install_dependencies()\n",
    "    prepare_environment()\n",
    "    tqdm.write(\"Main process completed.\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84494db7cf14dc090bdd4243671fdc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', options={'gsdf/CounterfeitXL': 'https://huggingface.co/gsdf/CounterfeitXL/resol…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97426d5bde904847acbaa1a5a405eafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='VAE:', options={'None': '', 'Original VAE': 'https://huggingface.co/stabilityai/sdxl-vae…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996fb9e056d24438a20640368e83f44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Password(description='Token:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2943cb397fe0482a9056e061d1bf0200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Download', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f155b5d13f354a37b6051b2bba7d1449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Button, Dropdown, Password, Output\n",
    "from IPython.display import display\n",
    "import requests\n",
    "\n",
    "# Model and VAE URLs for demonstration (specifically as you provided)\n",
    "MODEL_URLS = {\n",
    "    \"gsdf/CounterfeitXL\": \"https://huggingface.co/gsdf/CounterfeitXL/resolve/main/CounterfeitXL_%CE%B2.safetensors\",\n",
    "    \"Linaqruf/animagine-xl\": \"https://huggingface.co/Linaqruf/animagine-xl/resolve/main/animagine-xl.safetensors\",\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\": \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\",\n",
    "    \"stablediffusionapi/newreality-xl\": \"https://huggingface.co/stablediffusionapi/newreality-xl/blob/main/unet/diffusion_pytorch_model.bin\",\n",
    "    \"stablediffusionapi/newrealityxl-global-nsfw\": \"https://huggingface.co/stablediffusionapi/newrealityxl-global-nsfw/blob/main/unet/diffusion_pytorch_model.safetensors\"\n",
    "}\n",
    "\n",
    "VAE_URLS = {\n",
    "    \"None\": \"\",\n",
    "    \"Original VAE\": \"https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors\",\n",
    "    \"FP16 VAE\": \"https://huggingface.co/madebyollin/sdxl-vae-fp16-fix/resolve/main/sdxl_vae.safetensors\",\n",
    "    \"new reality\": \"https://huggingface.co/stablediffusionapi/newrealityxl-global-nsfw/blob/main/vae/diffusion_pytorch_model.safetensors\"\n",
    "}\n",
    "\n",
    "# Setup widgets\n",
    "model_dropdown = Dropdown(options=MODEL_URLS, description='Model:')\n",
    "vae_dropdown = Dropdown(options=VAE_URLS, description='VAE:')\n",
    "token_input = Password(description=\"Token:\")\n",
    "download_button = Button(description='Download')\n",
    "output_area = Output()\n",
    "\n",
    "# Display Widgets\n",
    "display(model_dropdown, vae_dropdown, token_input, download_button, output_area)\n",
    "\n",
    "def download_file(url):\n",
    "    \"\"\"Function to download files with streaming.\"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        # Assuming '/home/jovyan' is the path where files should be saved\n",
    "        path = f\"/home/jovyan/{url.split('/')[-1]}\"\n",
    "        with open(path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        return f\"Downloaded from {url}\"\n",
    "    else:\n",
    "        return f\"Failed to download from {url}\"\n",
    "\n",
    "@download_button.on_click\n",
    "def on_button_clicked(b):\n",
    "    \"\"\"Handle button click event.\"\"\"\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        if not token_input.value:\n",
    "            print(\"Token is required!\")\n",
    "        else:\n",
    "            model_url = model_dropdown.value  # Corrected from 'model' to 'model_dropdown.value'\n",
    "            vae_url = vae_dropdown.value\n",
    "            print(f\"Starting downloads with token: {token_input.value[:4]}...\")  # Only show the first 4 characters of the token\n",
    "            model_result = download_file(model_url)\n",
    "            vae_result = download_file(vae_url)\n",
    "            print(model_result)\n",
    "            print(vae_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1714750740639,
     "user": {
      "displayName": "Sung Kim",
      "userId": "16507898750669832188"
     },
     "user_tz": -540
    },
    "id": "kh7CeDqK4l3Y",
    "outputId": "5ed6f7b7-d3a1-4926-8fba-9585d805d3f6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd699016dfc440ea6103085e5c6d19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='/mnt/myhdd/train/sungkima', description='Source Directory:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231697fe2dd94a598ad472fd531e25d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='/home/jovyan/train', description='Base Target Directory:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d9d10decab452db070267a51ef42d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Create Subfolder and Transfer Data', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f59e2b9f6ec4a688c971a2e82c4a787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Button, Text, Output\n",
    "from IPython.display import display\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Setup widgets for input\n",
    "source_dir_input = Text(value='/mnt/myhdd/train/sungkima', description='Source Directory:')\n",
    "base_target_dir_input = Text(value='/home/jovyan/train', description='Base Target Directory:')\n",
    "transfer_button = Button(description='Create Subfolder and Transfer Data')\n",
    "output_area = Output()\n",
    "\n",
    "# Display widgets for user interaction\n",
    "display(source_dir_input, base_target_dir_input, transfer_button, output_area)\n",
    "\n",
    "def transfer_files(source_dir, target_dir):\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    for filename in os.listdir(source_dir):\n",
    "        source_file = os.path.join(source_dir, filename)\n",
    "        target_file = os.path.join(target_dir, filename)\n",
    "        if not os.path.exists(target_file):\n",
    "            shutil.copy(source_file, target_file)\n",
    "        else:\n",
    "            print(f\"File already exists: {target_file}\")\n",
    "\n",
    "@transfer_button.on_click\n",
    "def on_transfer_clicked(b):\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        source_dir = source_dir_input.value\n",
    "        base_target_dir = base_target_dir_input.value\n",
    "        subfolder_name = os.path.basename(os.path.normpath(source_dir))\n",
    "        target_dir = os.path.join(base_target_dir, subfolder_name)\n",
    "        if not os.path.exists(source_dir):\n",
    "            print(\"Source directory does not exist.\")\n",
    "            return\n",
    "        transfer_files(source_dir, target_dir)\n",
    "        print(f\"Data transferred from {source_dir} to {target_dir}\")\n",
    "        %store subfolder_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r subfolder_name\n",
    "base_target_dir = '/home/jovyan/train'\n",
    "train_data_dir = os.path.join(base_target_dir, subfolder_name)\n",
    "\n",
    "# Now `train_data_dir` can be used in your processing scripts\n",
    "print(\"Training data directory:\", train_data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-0qKyEgTchp"
   },
   "source": [
    "# **III. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3517,
     "status": "ok",
     "timestamp": 1714615725214,
     "user": {
      "displayName": "Sung Kim",
      "userId": "16507898750669832188"
     },
     "user_tz": -540
    },
    "id": "Jz2emq6vWnPu",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "f1c3d623-ed2d-41e5-ffe6-88030111c0d8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 818.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images have been converted\n"
     ]
    }
   ],
   "source": [
    "# @title ## **3.1. Data Cleaning**\n",
    "import os\n",
    "import random\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "%store -r\n",
    "\n",
    "os.chdir(root_dir)\n",
    "\n",
    "test = os.listdir(train_data_dir)\n",
    "#@markdown This section removes unsupported media types such as `.mp4`, `.webm`, and `.gif`, as well as any unnecessary files.\n",
    "#@markdown To convert a transparent dataset with an alpha channel (RGBA) to RGB and give it a white background, set the `convert` parameter to `True`.\n",
    "convert = True  # @param {type:\"boolean\"}\n",
    "#@markdown Alternatively, you can give the background a `random_color` instead of white by checking the corresponding option.\n",
    "random_color = True  # @param {type:\"boolean\"}\n",
    "recursive = False\n",
    "\n",
    "batch_size = 32\n",
    "supported_types = [\n",
    "    \".png\",\n",
    "    \".jpg\",\n",
    "    \".jpeg\",\n",
    "    \".webp\",\n",
    "    \".bmp\",\n",
    "    \".caption\",\n",
    "    \".npz\",\n",
    "    \".txt\",\n",
    "    \".json\",\n",
    "]\n",
    "\n",
    "background_colors = [\n",
    "    (255, 255, 255),\n",
    "    (0, 0, 0),\n",
    "    (255, 0, 0),\n",
    "    (0, 255, 0),\n",
    "    (0, 0, 255),\n",
    "    (255, 255, 0),\n",
    "    (255, 0, 255),\n",
    "    (0, 255, 255),\n",
    "]\n",
    "\n",
    "def clean_directory(directory):\n",
    "    for item in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, item)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_ext = os.path.splitext(item)[1]\n",
    "            if file_ext not in supported_types:\n",
    "                print(f\"Deleting file {item} from {directory}\")\n",
    "                os.remove(file_path)\n",
    "        elif os.path.isdir(file_path) and recursive:\n",
    "            clean_directory(file_path)\n",
    "\n",
    "def process_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img_dir, image_name = os.path.split(image_path)\n",
    "\n",
    "    if img.mode in (\"RGBA\", \"LA\"):\n",
    "        if random_color:\n",
    "            background_color = random.choice(background_colors)\n",
    "        else:\n",
    "            background_color = (255, 255, 255)\n",
    "        bg = Image.new(\"RGB\", img.size, background_color)\n",
    "        bg.paste(img, mask=img.split()[-1])\n",
    "\n",
    "        if image_name.endswith(\".webp\"):\n",
    "            bg = bg.convert(\"RGB\")\n",
    "            new_image_path = os.path.join(img_dir, image_name.replace(\".webp\", \".jpg\"))\n",
    "            bg.save(new_image_path, \"JPEG\")\n",
    "            os.remove(image_path)\n",
    "            print(f\" Converted image: {image_name} to {os.path.basename(new_image_path)}\")\n",
    "        else:\n",
    "            bg.save(image_path, \"PNG\")\n",
    "            print(f\" Converted image: {image_name}\")\n",
    "    else:\n",
    "        if image_name.endswith(\".webp\"):\n",
    "            new_image_path = os.path.join(img_dir, image_name.replace(\".webp\", \".jpg\"))\n",
    "            img.save(new_image_path, \"JPEG\")\n",
    "            os.remove(image_path)\n",
    "            print(f\" Converted image: {image_name} to {os.path.basename(new_image_path)}\")\n",
    "        else:\n",
    "            img.save(image_path, \"PNG\")\n",
    "\n",
    "def find_images(directory):\n",
    "    images = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".png\") or file.endswith(\".webp\"):\n",
    "                images.append(os.path.join(root, file))\n",
    "    return images\n",
    "\n",
    "clean_directory(train_data_dir)\n",
    "images = find_images(train_data_dir)\n",
    "num_batches = len(images) // batch_size + 1\n",
    "\n",
    "if convert:\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        for i in tqdm(range(num_batches)):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            batch = images[start:end]\n",
    "            executor.map(process_image, batch)\n",
    "\n",
    "    print(\"All images have been converted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdISafLeyklg"
   },
   "source": [
    "## **3.2. Data Captioning**\n",
    "\n",
    "- For general images, use BLIP captioning.\n",
    "- For anime and manga-style images, use Waifu Diffusion 1.4 Tagger V2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvPyH-G_Qdha",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title ### **3.2.1. BLIP Captioning**\n",
    "#@markdown BLIP is a pre-training framework for unified vision-language understanding and generation, which achieves state-of-the-art results on a wide range of vision-language tasks. It can be used as a tool for image captioning, for example, `astronaut riding a horse in space`.\n",
    "import os\n",
    "\n",
    "os.chdir(finetune_dir)\n",
    "\n",
    "beam_search = True #@param {type:'boolean'}\n",
    "min_length = 5 #@param {type:\"slider\", min:0, max:100, step:5.0}\n",
    "max_length = 90 #@param {type:\"slider\", min:0, max:100, step:5.0}\n",
    "\n",
    "config = {\n",
    "    \"_train_data_dir\"   : train_data_dir,\n",
    "    \"batch_size\"        : 8,\n",
    "    \"beam_search\"       : beam_search,\n",
    "    \"min_length\"        : min_length,\n",
    "    \"max_length\"        : max_length,\n",
    "    \"debug\"             : True,\n",
    "    \"caption_extension\" : \".caption\",\n",
    "    \"max_data_loader_n_workers\" : 2,\n",
    "    \"recursive\"         : True\n",
    "}\n",
    "\n",
    "args = \"\"\n",
    "for k, v in config.items():\n",
    "    if k.startswith(\"_\"):\n",
    "        args += f'\"{v}\" '\n",
    "    elif isinstance(v, str):\n",
    "        args += f'--{k}=\"{v}\" '\n",
    "    elif isinstance(v, bool) and v:\n",
    "        args += f\"--{k} \"\n",
    "    elif isinstance(v, float) and not isinstance(v, bool):\n",
    "        args += f\"--{k}={v} \"\n",
    "    elif isinstance(v, int) and not isinstance(v, bool):\n",
    "        args += f\"--{k}={v} \"\n",
    "\n",
    "final_args = f\"python make_captions.py {args}\"\n",
    "\n",
    "os.chdir(finetune_dir)\n",
    "!{final_args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rHkSBphHbcE",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Retrieve the stored training data directory\n",
    "%store -r train_data_dir\n",
    "\n",
    "# Colab form fields for interactive input\n",
    "file_name = \"\" #@param {type:\"string\"}\n",
    "original_text = \"a man \" #@param {type:\"string\"}\n",
    "replacement_text = \"whj \" #@param {type:\"string\"}\n",
    "include_subdirectories = True #@param {type:\"boolean\"}\n",
    "\n",
    "def enhanced_text_replacement(directory_path=train_data_dir, file_name=\"\", original_text=\"Enter original text\", replacement_text=\"Enter replacement text\", include_subdirectories=False):\n",
    "    print(\"Starting enhanced text replacement utility.\")\n",
    "\n",
    "    processed_files = 0\n",
    "\n",
    "    if include_subdirectories:\n",
    "        # Process files in the directory and its subdirectories\n",
    "        for dirpath, dirnames, files in os.walk(directory_path):\n",
    "            for f in files:\n",
    "                if f.endswith('.caption'):\n",
    "                    full_path = os.path.join(dirpath, f)\n",
    "                    process_file(full_path, original_text, replacement_text)\n",
    "                    processed_files += 1\n",
    "    else:\n",
    "        if file_name == \"\":\n",
    "            # Apply changes to all .caption files in the directory\n",
    "            files = [f for f in os.listdir(directory_path) if f.endswith('.caption')]\n",
    "        else:\n",
    "            # Apply changes to the specified file only\n",
    "            files = [file_name] if file_name.endswith('.caption') else [file_name + '.caption']\n",
    "\n",
    "        for filename in files:\n",
    "            full_path = os.path.join(directory_path, filename)\n",
    "            process_file(full_path, original_text, replacement_text)\n",
    "            processed_files += 1\n",
    "\n",
    "    print(f\"Total files updated: {processed_files}\")\n",
    "\n",
    "def process_file(file_path, original_text, replacement_text):\n",
    "    if os.path.isfile(file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "            content = content.replace(original_text, replacement_text)\n",
    "            with open(file_path, 'w') as file:\n",
    "                file.write(content)\n",
    "            print(f\"Processed {file_path}\")\n",
    "            # Print the updated contents of the file\n",
    "            print(content)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Function call\n",
    "enhanced_text_replacement(file_name=file_name, original_text=original_text, replacement_text=replacement_text, include_subdirectories=include_subdirectories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-BdXV7rAy2ag",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title ### **3.2.2. Waifu Diffusion 1.4 Tagger V2**\n",
    "import os\n",
    "%store -r\n",
    "\n",
    "os.chdir(finetune_dir)\n",
    "\n",
    "#@markdown [Waifu Diffusion 1.4 Tagger V2](https://huggingface.co/spaces/SmilingWolf/wd-v1-4-tags) is a Danbooru-styled image classification model developed by SmilingWolf. It can also be useful for general image tagging, for example, `1girl, solo, looking_at_viewer, short_hair, bangs, simple_background`.\n",
    "model = \"SmilingWolf/wd-v1-4-convnextv2-tagger-v2\" #@param [\"SmilingWolf/wd-v1-4-moat-tagger-v2\", \"SmilingWolf/wd-v1-4-convnextv2-tagger-v2\", \"SmilingWolf/wd-v1-4-swinv2-tagger-v2\", \"SmilingWolf/wd-v1-4-convnext-tagger-v2\", \"SmilingWolf/wd-v1-4-vit-tagger-v2\"]\n",
    "#@markdown Separate `undesired_tags` with comma `(,)` if you want to remove multiple tags, e.g. `1girl,solo,smile`.\n",
    "undesired_tags = \"\" #@param {type:'string'}\n",
    "#@markdown Adjust `general_threshold` for pruning tags (less tags, less flexible). `character_threshold` is useful if you want to train with character tags, e.g. `hakurei reimu`.\n",
    "general_threshold = 0.75 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "character_threshold = 0.7 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
    "\n",
    "config = {\n",
    "    \"_train_data_dir\"           : train_data_dir,\n",
    "    \"batch_size\"                : 8,\n",
    "    \"repo_id\"                   : model,\n",
    "    \"recursive\"                 : True,\n",
    "    \"remove_underscore\"         : True,\n",
    "    \"general_threshold\"         : general_threshold,\n",
    "    \"character_threshold\"       : character_threshold,\n",
    "    \"caption_extension\"         : \".txt\",\n",
    "    \"max_data_loader_n_workers\" : 2,\n",
    "    \"debug\"                     : True,\n",
    "    \"undesired_tags\"            : undesired_tags\n",
    "}\n",
    "\n",
    "args = \"\"\n",
    "for k, v in config.items():\n",
    "    if k.startswith(\"_\"):\n",
    "        args += f'\"{v}\" '\n",
    "    elif isinstance(v, str):\n",
    "        args += f'--{k}=\"{v}\" '\n",
    "    elif isinstance(v, bool) and v:\n",
    "        args += f\"--{k} \"\n",
    "    elif isinstance(v, float) and not isinstance(v, bool):\n",
    "        args += f\"--{k}={v} \"\n",
    "    elif isinstance(v, int) and not isinstance(v, bool):\n",
    "        args += f\"--{k}={v} \"\n",
    "\n",
    "final_args = f\"python tag_images_by_wd14_tagger.py {args}\"\n",
    "\n",
    "os.chdir(finetune_dir)\n",
    "!{final_args}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k10WYGu6Zt1q"
   },
   "source": [
    "# **Adding Text in front**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "axsIca0Nnxhr",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title ### **Prepend Text to Files Function**\n",
    "import os\n",
    "\n",
    "%store -r train_data_dir\n",
    "\n",
    "include_subdirectories = True #@param {type:\"boolean\"}\n",
    "text_to_prepend = \"whj, \" #@param {type:\"string\"}\n",
    "\n",
    "# Function to prepend text to all files in a directory\n",
    "def prepend_text_to_files(directory_path=train_data_dir, text_to_prepend=\"Enter text to prepend\", include_subdirectories=False):\n",
    "    print(\"Starting text prepend utility.\")\n",
    "\n",
    "    processed_files = 0\n",
    "    if include_subdirectories:\n",
    "        # Process files in the directory and its subdirectories\n",
    "        for dirpath, dirnames, files in os.walk(directory_path):\n",
    "            for filename in files:\n",
    "                if filename.endswith('.txt'):\n",
    "                    full_path = os.path.join(dirpath, filename)\n",
    "                    prepend_text(full_path, text_to_prepend)\n",
    "                    processed_files += 1\n",
    "    else:\n",
    "        # Iterate over each file in the specified directory only\n",
    "        for filename in os.listdir(directory_path):\n",
    "            if filename.endswith('.txt'):\n",
    "                full_path = os.path.join(directory_path, filename)\n",
    "                prepend_text(full_path, text_to_prepend)\n",
    "                processed_files += 1\n",
    "\n",
    "    print(f\"All files have been processed. Total files updated: {processed_files}\")\n",
    "\n",
    "def prepend_text(file_path, text_to_prepend):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            original_content = file.read()\n",
    "\n",
    "        new_content = text_to_prepend + original_content\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(new_content)\n",
    "\n",
    "        print(f\"Processed {file_path}\")\n",
    "        print(f\"Prepended text: {text_to_prepend}\")\n",
    "    except IOError as e:\n",
    "        print(f\"An error occurred while processing {file_path}: {e}\")\n",
    "\n",
    "# Run the function with default arguments\n",
    "prepend_text_to_files(text_to_prepend=text_to_prepend, include_subdirectories=include_subdirectories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_mLVURhM9PFE",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title ### **3.2.3. Custom Caption/Tag**\n",
    "import os\n",
    "\n",
    "%store -r\n",
    "\n",
    "os.chdir(root_dir)\n",
    "\n",
    "# @markdown Add or remove custom tags here.\n",
    "extension   = \".txt\"  # @param [\".txt\", \".caption\"]\n",
    "custom_tag  = \"circumcised,\"  # @param {type:\"string\"}\n",
    "# @markdown Use `sub_folder` option to specify a subfolder for multi-concept training.\n",
    "# @markdown > Specify `--all` to process all subfolders/`recursive`\n",
    "sub_folder  = \"--all\" #@param {type: \"string\"}\n",
    "# @markdown Enable this to append custom tags at the end of lines.\n",
    "append      = True  # @param {type:\"boolean\"}\n",
    "# @markdown Enable this if you want to remove captions/tags instead.\n",
    "remove_tag  = False  # @param {type:\"boolean\"}\n",
    "recursive   = False\n",
    "\n",
    "if sub_folder == \"\":\n",
    "    image_dir = train_data_dir\n",
    "elif sub_folder == \"--all\":\n",
    "    image_dir = train_data_dir\n",
    "    recursive = True\n",
    "elif sub_folder.startswith(\"/content\"):\n",
    "    image_dir = sub_folder\n",
    "else:\n",
    "    image_dir = os.path.join(train_data_dir, sub_folder)\n",
    "    os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        contents = f.read()\n",
    "    return contents\n",
    "\n",
    "def write_file(filename, contents):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(contents)\n",
    "\n",
    "def process_tags(filename, custom_tag, append, remove_tag):\n",
    "    contents = read_file(filename)\n",
    "    tags = [tag.strip() for tag in contents.split(',')]\n",
    "    custom_tags = [tag.strip() for tag in custom_tag.split(',')]\n",
    "\n",
    "    for custom_tag in custom_tags:\n",
    "        custom_tag = custom_tag.replace(\"_\", \" \")\n",
    "        if remove_tag:\n",
    "            while custom_tag in tags:\n",
    "                tags.remove(custom_tag)\n",
    "        else:\n",
    "            if custom_tag not in tags:\n",
    "                if append:\n",
    "                    tags.append(custom_tag)\n",
    "                else:\n",
    "                    tags.insert(0, custom_tag)\n",
    "\n",
    "    contents = ', '.join(tags)\n",
    "    write_file(filename, contents)\n",
    "\n",
    "def process_directory(image_dir, tag, append, remove_tag, recursive):\n",
    "    for filename in os.listdir(image_dir):\n",
    "        file_path = os.path.join(image_dir, filename)\n",
    "\n",
    "        if os.path.isdir(file_path) and recursive:\n",
    "            process_directory(file_path, tag, append, remove_tag, recursive)\n",
    "        elif filename.endswith(extension):\n",
    "            process_tags(file_path, tag, append, remove_tag)\n",
    "\n",
    "tag = custom_tag\n",
    "\n",
    "if not any(\n",
    "    [filename.endswith(extension) for filename in os.listdir(image_dir)]\n",
    "):\n",
    "    for filename in os.listdir(image_dir):\n",
    "        if filename.endswith((\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")):\n",
    "            open(\n",
    "                os.path.join(image_dir, filename.split(\".\")[0] + extension),\n",
    "                \"w\",\n",
    "            ).close()\n",
    "\n",
    "if custom_tag:\n",
    "    process_directory(image_dir, tag, append, remove_tag, recursive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAZVkLuaRJ9e"
   },
   "source": [
    "# **IV. Training**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 80176,
     "status": "ok",
     "timestamp": 1714750820809,
     "user": {
      "displayName": "Sung Kim",
      "userId": "16507898750669832188"
     },
     "user_tz": -540
    },
    "id": "hhgatqF3leHJ",
    "outputId": "d467e797-ab29-419d-f5e2-a7f8e84928c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no stored variable or alias subfolder_name\n",
      "no stored variable or alias #\n",
      "no stored variable or alias Retrieve\n",
      "no stored variable or alias the\n",
      "no stored variable or alias stored\n",
      "no stored variable or alias subfolder\n",
      "no stored variable or alias name\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'subfolder_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-r subfolder_name  # Retrieve the stored subfolder name\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m base_target_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jovyan/train\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m train_data_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_target_dir, \u001b[43msubfolder_name\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining data directory:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_data_dir)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Define JSON paths and configurations\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subfolder_name' is not defined"
     ]
    }
   ],
   "source": [
    "%store -r subfolder_name  # Retrieve the stored subfolder name\n",
    "base_target_dir = '/home/jovyan/train'\n",
    "train_data_dir = os.path.join(base_target_dir, subfolder_name)\n",
    "print(\"Training data directory:\", train_data_dir)\n",
    "\n",
    "# Define JSON paths and configurations\n",
    "metadata_json = os.path.join(train_data_dir, \"meta_clean.json\")\n",
    "bucketing_json = os.path.join(train_data_dir, \"meta_lat.json\")\n",
    "bucket_resolution = 1024  # Slider control replaced by direct assignment\n",
    "mixed_precision = \"no\"\n",
    "skip_existing = True\n",
    "flip_aug = True\n",
    "clean_caption = True\n",
    "recursive = True\n",
    "\n",
    "# Configuration dictionaries for metadata merging and bucket preparation\n",
    "metadata_config = {\n",
    "    \"_train_data_dir\": train_data_dir,\n",
    "    \"_out_json\": metadata_json,\n",
    "    \"recursive\": recursive,\n",
    "    \"full_path\": recursive,\n",
    "    \"clean_caption\": clean_caption\n",
    "}\n",
    "\n",
    "bucketing_config = {\n",
    "    \"_train_data_dir\": train_data_dir,\n",
    "    \"_in_json\": metadata_json,\n",
    "    \"_out_json\": bucketing_json,\n",
    "    \"_model_name_or_path\": 'vae_path' if 'vae_path' in locals() else 'model_path',\n",
    "    \"recursive\": recursive,\n",
    "    \"full_path\": recursive,\n",
    "    \"flip_aug\": flip_aug,\n",
    "    \"skip_existing\": skip_existing,\n",
    "    \"batch_size\": 4,\n",
    "    \"max_data_loader_n_workers\": 2,\n",
    "    \"max_resolution\": f\"{bucket_resolution}, {bucket_resolution}\",\n",
    "    \"mixed_precision\": mixed_precision,\n",
    "}\n",
    "\n",
    "def generate_args(config):\n",
    "    args = \"\"\n",
    "    for k, v in config.items():\n",
    "        if k.startswith(\"_\"):\n",
    "            args += f'\"{v}\" '\n",
    "        elif isinstance(v, str):\n",
    "            args += f'--{k}=\"{v}\" '\n",
    "        elif isinstance(v, bool) and v:\n",
    "            args += f\"--{k} \"\n",
    "        elif isinstance(v, float) and not isinstance(v, bool):\n",
    "            args += f\"--{k}={v} \"\n",
    "        elif isinstance(v, int) and not isinstance(v, bool):\n",
    "            args += f\"--{k}={v} \"\n",
    "    return args.strip()\n",
    "\n",
    "merge_metadata_args = generate_args(metadata_config)\n",
    "prepare_buckets_args = generate_args(bucketing_config)\n",
    "\n",
    "# Commands to execute Python scripts\n",
    "merge_metadata_command = f\"python merge_all_to_metadata.py {merge_metadata_args}\"\n",
    "prepare_buckets_command = f\"python prepare_buckets_latents.py {prepare_buckets_args}\"\n",
    "\n",
    "# Assuming finetune_dir is defined and correctly set\n",
    "finetune_dir = '/path/to/your/finetuning/scripts'\n",
    "os.chdir(finetune_dir)  # Change to the directory containing the scripts\n",
    "!{merge_metadata_command}\n",
    "time.sleep(1)  # Small delay to ensure filesystem sync\n",
    "!{prepare_buckets_command}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1714750820809,
     "user": {
      "displayName": "Sung Kim",
      "userId": "16507898750669832188"
     },
     "user_tz": -540
    },
    "id": "cJgLfRtlHSjw",
    "outputId": "40a1fbc8-4936-4fa8-cc0e-08ddcda051ad"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c53f3f63744a0580024f80814700b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Network Category:', options=('LoRA_LierLa', 'LoRA_C3Lier', 'DyLoRA_LierLa', 'DyLoRA_C3Li…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac904983a42240e88d143a7059da0363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Network Args:', placeholder='Enter args like [\"args=1\", \"args=2\"]', style=TextStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b3a38091c249e5b92ae5344acbd32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=64, description='Network Dim:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa38a5e3dc814e438d853829d90fd3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=32, description='Network Alpha:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258c9a7496a84cad8ae4d67555d0c7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=8, description='Conv Dim:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cf4cd9f7884822a85403f8ff9af307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=1, description='Conv Alpha:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b191685a1b274059aa790ed3c0068807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=2, description='Unit:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714b70498eb743ad8eef32c6fbf44ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Configuration', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b45a7e5a0f4c48b39d4b552f4dc731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import ast\n",
    "import toml\n",
    "\n",
    "# Dropdown for selecting network category\n",
    "network_category_dropdown = widgets.Dropdown(\n",
    "    options=[\"LoRA_LierLa\", \"LoRA_C3Lier\", \"DyLoRA_LierLa\", \"DyLoRA_C3Lier\", \"LoCon\", \"LoHa\", \"IA3\", \"LoKR\", \"DyLoRA_Lycoris\"],\n",
    "    value='LoRA_LierLa',\n",
    "    description='Network Category:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Text input for optional training arguments\n",
    "network_args_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter args like [\"args=1\", \"args=2\"]',\n",
    "    description='Network Args:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Numeric input fields for network and convolution parameters\n",
    "network_dim_input = widgets.IntText(value=64, description='Network Dim:', style={'description_width': 'initial'})\n",
    "network_alpha_input = widgets.IntText(value=32, description='Network Alpha:', style={'description_width': 'initial'})\n",
    "conv_dim_input = widgets.IntText(value=8, description='Conv Dim:', style={'description_width': 'initial'})\n",
    "conv_alpha_input = widgets.IntText(value=1, description='Conv Alpha:', style={'description_width': 'initial'})\n",
    "unit_input = widgets.IntText(value=2, description='Unit:', style={'description_width': 'initial'})\n",
    "\n",
    "# Button to process the inputs and generate configuration\n",
    "process_button = widgets.Button(description=\"Generate Configuration\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "display(network_category_dropdown, network_args_input, network_dim_input, network_alpha_input, conv_dim_input, conv_alpha_input, unit_input, process_button, output_area)\n",
    "\n",
    "def process_configuration(b):\n",
    "    network_args = network_args_input.value.strip()\n",
    "    if network_args.startswith('[') and network_args.endswith(']'):\n",
    "        try:\n",
    "            network_args = ast.literal_eval(network_args)\n",
    "        except (SyntaxError, ValueError) as e:\n",
    "            network_args = []\n",
    "            print(f\"Error parsing network_args: {e}\\n\")\n",
    "    elif len(network_args) > 0:\n",
    "        print(f\"WARNING! '{network_args}' is not a valid list! Put args like this: [\\\"args=1\\\", \\\"args=2\\\"]\\n\")\n",
    "        network_args = []\n",
    "    else:\n",
    "        network_args = []\n",
    "\n",
    "    # Define network configuration based on inputs\n",
    "    network_category = network_category_dropdown.value\n",
    "    network_dim = network_dim_input.value\n",
    "    network_alpha = network_alpha_input.value\n",
    "    conv_dim = conv_dim_input.value\n",
    "    conv_alpha = conv_alpha_input.value\n",
    "    unit = unit_input.value\n",
    "\n",
    "    # Configuration logic\n",
    "    network_config = {\n",
    "        \"LoRA_LierLa\": {\"module\": \"networks.lora\", \"args\": []},\n",
    "        \"LoRA_C3Lier\": {\"module\": \"networks.lora\", \"args\": [f\"conv_dim={conv_dim}\", f\"conv_alpha={conv_alpha}\"]},\n",
    "        # Additional configurations as per original script\n",
    "    }\n",
    "    network_module = network_config[network_category][\"module\"]\n",
    "    network_args.extend(network_config[network_category][\"args\"])\n",
    "\n",
    "    lora_config = {\n",
    "        \"additional_network_arguments\": {\n",
    "            \"no_metadata\": False,\n",
    "            \"network_module\": network_module,\n",
    "            \"network_dim\": network_dim,\n",
    "            \"network_alpha\": network_alpha,\n",
    "            \"network_args\": network_args,\n",
    "            \"network_train_unet_only\": True,\n",
    "            \"training_comment\": None,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        print(toml.dumps(lora_config))\n",
    "\n",
    "process_button.on_click(process_configuration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1714774657331,
     "user": {
      "displayName": "Sung Kim",
      "userId": "16507898750669832188"
     },
     "user_tz": -540
    },
    "id": "JNlw3u8arwir",
    "outputId": "3566433c-d32d-4514-f9ff-3a16ac0a82e2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9869c28b5b3c44ba83a4792ea82fecf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Optimizer Type:', index=13, options=('AdamW', 'AdamW8bit', 'Lion8bit', 'Lion', 'SGDNeste…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a7967a0d364350956c1781beb2c182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='[ \"scale_parameter=False\", \"relative_step=False\", \"warmup_init=False\" ]', description='Optimizer A…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7eaea1f5944a02a5a97bf2c77bea98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=8e-06, description='Learning Rate:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0841d7305440c79066539f85d15363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=1.0, description='Max Grad Norm:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99167be67284dcd8f4dd86419174350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='LR Scheduler:', index=3, options=('linear', 'cosine', 'cosine_with_restarts', 'polynomia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c0eca5e3fa40fc9b7af12a01fdd4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=300, description='LR Warmup Steps:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f28b3322c0447f94d3deca2d66ab71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=3, description='LR Scheduler Num:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337e8cf4f0734e14a7be711292587214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Configuration', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717bbc96d4104952972bba7a6830ac3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import ast\n",
    "import toml\n",
    "\n",
    "# Dropdown for selecting optimizer type\n",
    "optimizer_type_dropdown = widgets.Dropdown(\n",
    "    options=[\"AdamW\", \"AdamW8bit\", \"Lion8bit\", \"Lion\", \"SGDNesterov\", \"SGDNesterov8bit\", \"DAdaptation(DAdaptAdamPreprint)\", \"DAdaptAdaGrad\", \"DAdaptAdam\", \"DAdaptAdan\", \"DAdaptAdanIP\", \"DAdaptLion\", \"DAdaptSGD\", \"AdaFactor\"],\n",
    "    value='AdaFactor',\n",
    "    description='Optimizer Type:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Text input for additional optimizer arguments\n",
    "optimizer_args_input = widgets.Text(\n",
    "    value='[ \"scale_parameter=False\", \"relative_step=False\", \"warmup_init=False\" ]',\n",
    "    placeholder='Enter args like [\"arg1=value\", \"arg2=value\"]',\n",
    "    description='Optimizer Args:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Numeric inputs for learning rate and gradient norm configuration\n",
    "learning_rate_input = widgets.FloatText(value=8e-6, description='Learning Rate:', style={'description_width': 'initial'})\n",
    "max_grad_norm_input = widgets.FloatText(value=1, description='Max Grad Norm:', style={'description_width': 'initial'})\n",
    "\n",
    "# Dropdown for selecting the learning rate scheduler\n",
    "lr_scheduler_dropdown = widgets.Dropdown(\n",
    "    options=[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\", \"adafactor\"],\n",
    "    value='polynomial',\n",
    "    description='LR Scheduler:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Numeric input for LR warmup steps and scheduler number\n",
    "lr_warmup_steps_input = widgets.IntText(value=300, description='LR Warmup Steps:', style={'description_width': 'initial'})\n",
    "lr_scheduler_num_input = widgets.IntText(value=3, description='LR Scheduler Num:', style={'description_width': 'initial'})\n",
    "\n",
    "# Button to process the inputs and generate configuration\n",
    "process_button = widgets.Button(description=\"Generate Configuration\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "display(optimizer_type_dropdown, optimizer_args_input, learning_rate_input, max_grad_norm_input, lr_scheduler_dropdown, lr_warmup_steps_input, lr_scheduler_num_input, process_button, output_area)\n",
    "\n",
    "def process_configuration(b):\n",
    "    # Parse optimizer arguments from string to list\n",
    "    optimizer_args = optimizer_args_input.value.strip()\n",
    "    if optimizer_args.startswith('[') and optimizer_args.endswith(']'):\n",
    "        try:\n",
    "            optimizer_args = ast.literal_eval(optimizer_args)\n",
    "        except (SyntaxError, ValueError) as e:\n",
    "            print(f\"Error parsing optimizer_args: {e}\")\n",
    "            optimizer_args = []\n",
    "    else:\n",
    "        optimizer_args = []\n",
    "\n",
    "    # Gather all configurations\n",
    "    optimizer_config = {\n",
    "        \"optimizer_arguments\": {\n",
    "            \"optimizer_type\": optimizer_type_dropdown.value,\n",
    "            \"learning_rate\": learning_rate_input.value,\n",
    "            \"max_grad_norm\": max_grad_norm_input.value,\n",
    "            \"optimizer_args\": optimizer_args,\n",
    "            \"lr_scheduler\": lr_scheduler_dropdown.value,\n",
    "            \"lr_warmup_steps\": lr_warmup_steps_input.value,\n",
    "            \"lr_scheduler_num_cycles\": lr_scheduler_num_input.value if lr_scheduler_dropdown.value == \"cosine_with_restarts\" else None,\n",
    "            \"lr_scheduler_power\": lr_scheduler_num_input.value if lr_scheduler_dropdown.value == \"polynomial\" else None,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        print(toml.dumps(optimizer_config))\n",
    "\n",
    "process_button.on_click(process_configuration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1714774647377,
     "user": {
      "displayName": "Sung Kim",
      "userId": "16507898750669832188"
     },
     "user_tz": -540
    },
    "id": "YOxNM0x7dvfO",
    "outputId": "2d9fe660-88c8-4d0c-ff4b-f47aefa535f5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69835392ee274c79bc9e61ca65d1ae58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Save Optimizer State')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9414bde2ed87427281c9e110a8b0c45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='/content/drive/MyDrive/kohya-trainer/output/whj2/whj2-state', description='Load State Path:', styl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac35305e227244a1856e8dbfaf6dbb5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Noise Control Type:', index=2, options=('none', 'noise_offset', 'multires_noise'), style…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f673944da25487580a0a6b86477838a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.1, description='Noise Offset:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66dfa8de18a74a59a6d618abeb4ab654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.01, description='Adaptive Noise Scale:', style=DescriptionStyle(description_width='initial')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba3e978759d42edbf202bf1b917d7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=10, description='Multires Iterations:', max=10, min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058e421ee91e4b63b44e51edebb45d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.8, description='Multires Discount:', max=1.0, min=0.1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb97490083c4462a8cf722dad345361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.1, description='Caption Dropout Rate:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358f3f5c9e664ec8865f5f240565c746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.1, description='Tag Dropout Rate:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8380a782cd7a437f8a7416bc93778f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=5, description='Dropout Every N Epochs:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d8a1535e2e406b9219db6b565f89ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=5.0, description='Min SNR Gamma:', style=DescriptionStyle(description_width='initial'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e863ba8359ef4da8928ef90c74de9e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Configuration', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc407e7f2144e0cadd9932a3fe6fdd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import toml\n",
    "\n",
    "# Checkbox for saving/loading optimizer state\n",
    "save_optimizer_state_checkbox = widgets.Checkbox(value=True, description='Save Optimizer State', disabled=False)\n",
    "load_optimizer_state_input = widgets.Text(value='/content/drive/MyDrive/kohya-trainer/output/whj2/whj2-state', description='Load State Path:', style={'description_width': 'initial'})\n",
    "\n",
    "# Dropdown for noise control type\n",
    "noise_control_dropdown = widgets.Dropdown(\n",
    "    options=['none', 'noise_offset', 'multires_noise'],\n",
    "    value='multires_noise',\n",
    "    description='Noise Control Type:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Numeric input for noise offset and adaptive scale\n",
    "noise_offset_input = widgets.FloatText(value=0.1, description='Noise Offset:', style={'description_width': 'initial'})\n",
    "adaptive_noise_scale_input = widgets.FloatText(value=0.01, description='Adaptive Noise Scale:', style={'description_width': 'initial'})\n",
    "\n",
    "# Slider for multires noise iterations and discount\n",
    "multires_noise_iterations_slider = widgets.IntSlider(value=10, min=1, max=10, step=1, description='Multires Iterations:')\n",
    "multires_noise_discount_slider = widgets.FloatSlider(value=0.8, min=0.1, max=1, step=0.1, description='Multires Discount:')\n",
    "\n",
    "# Caption dropout configuration\n",
    "caption_dropout_rate_input = widgets.FloatText(value=0.1, description='Caption Dropout Rate:', style={'description_width': 'initial'})\n",
    "caption_tag_dropout_rate_input = widgets.FloatText(value=0.1, description='Tag Dropout Rate:', style={'description_width': 'initial'})\n",
    "caption_dropout_every_n_epochs_input = widgets.IntText(value=5, description='Dropout Every N Epochs:', style={'description_width': 'initial'})\n",
    "\n",
    "# Gamma configuration\n",
    "min_snr_gamma_input = widgets.FloatText(value=5, description='Min SNR Gamma:', style={'description_width': 'initial'})\n",
    "\n",
    "# Button to process the inputs and generate configuration\n",
    "process_button = widgets.Button(description=\"Generate Configuration\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Display all widgets\n",
    "display(save_optimizer_state_checkbox, load_optimizer_state_input, noise_control_dropdown, \n",
    "        noise_offset_input, adaptive_noise_scale_input, multires_noise_iterations_slider, \n",
    "        multires_noise_discount_slider, caption_dropout_rate_input, caption_tag_dropout_rate_input, \n",
    "        caption_dropout_every_n_epochs_input, min_snr_gamma_input, process_button, output_area)\n",
    "\n",
    "def process_configuration(b):\n",
    "    advanced_training_config = {\n",
    "        \"advanced_training_config\": {\n",
    "            \"resume\": load_optimizer_state_input.value,\n",
    "            \"save_state\": save_optimizer_state_checkbox.value,\n",
    "            \"save_last_n_epochs_state\": save_optimizer_state_checkbox.value,\n",
    "            \"noise_offset\": noise_offset_input.value if noise_control_dropdown.value == \"noise_offset\" else None,\n",
    "            \"adaptive_noise_scale\": adaptive_noise_scale_input.value if adaptive_noise_scale_input.value and noise_control_dropdown.value == \"noise_offset\" else None,\n",
    "            \"multires_noise_iterations\": multires_noise_iterations_slider.value if noise_control_dropdown.value == \"multires_noise\" else None,\n",
    "            \"multires_noise_discount\": multires_noise_discount_slider.value if noise_control_dropdown.value == \"multires_noise\" else None,\n",
    "            \"caption_dropout_rate\": caption_dropout_rate_input.value,\n",
    "            \"caption_tag_dropout_rate\": caption_tag_dropout_rate_input.value,\n",
    "            \"caption_dropout_every_n_epochs\": caption_dropout_every_n_epochs_input.value,\n",
    "            \"min_snr_gamma\": min_snr_gamma_input.value if min_snr_gamma_input.value != -1 else None,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        print(toml.dumps(advanced_training_config))\n",
    "\n",
    "process_button.on_click(process_configuration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ihuf9xR12koI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no stored variable or alias subfolder_name\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'subfolder_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     subfolder_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault_subfolder\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Provide a default or an error handling routine\u001b[39;00m\n\u001b[1;32m     15\u001b[0m base_target_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jovyan/train\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 16\u001b[0m train_data_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_target_dir, \u001b[43msubfolder_name\u001b[49m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSingleFolderDataset\u001b[39;00m(Dataset):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subfolder_name' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Attempt to retrieve subfolder_name, ensure you have run the %store subfolder_name in another cell before this.\n",
    "try:\n",
    "    %store -r subfolder_name\n",
    "except NameError:\n",
    "    print(\"subfolder_name is not defined. Ensure you have stored it with '%store subfolder_name' after setting it.\")\n",
    "    subfolder_name = \"default_subfolder\"  # Provide a default or an error handling routine\n",
    "\n",
    "base_target_dir = '/home/jovyan/train'\n",
    "train_data_dir = os.path.join(base_target_dir, subfolder_name)\n",
    "\n",
    "class SingleFolderDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.images = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Define transformations and DataLoader settings\n",
    "transformations = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Widgets for data loader configuration\n",
    "batch_size_input = widgets.IntText(value=20, description='Batch Size:', style={'description_width': 'initial'})\n",
    "shuffle_checkbox = widgets.Checkbox(value=True, description='Shuffle:', disabled=False)\n",
    "num_workers_input = widgets.IntText(value=4, description='Num Workers:', style={'description_width': 'initial'})\n",
    "init_button = widgets.Button(description=\"Initialize Dataset and Loader\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "display(batch_size_input, shuffle_checkbox, num_workers_input, init_button, output_area)\n",
    "\n",
    "def initialize_dataset_and_loader(b):\n",
    "    dataset = SingleFolderDataset(directory=train_data_dir, transform=transformations)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size_input.value, shuffle=shuffle_checkbox.value, num_workers=num_workers_input.value)\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        print(f\"Dataset initialized with {len(dataset)} images.\")\n",
    "        print(f\"Data loader ready with batch size {batch_size_input.value}.\")\n",
    "\n",
    "init_button.on_click(initialize_dataset_and_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1714774714650,
     "user": {
      "displayName": "Sung Kim",
      "userId": "16507898750669832188"
     },
     "user_tz": -540
    },
    "id": "-Z4w3lfFKLjr",
    "outputId": "9b468e64-b44f-4a62-e116-0fcc145854bf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5481047130e4dbd86ff47d1621d1af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(Text(value='whj3', description='Project Name:', style=TextStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c062037209be42c3a052017d9e261200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate Training Configuration', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e08cd9aace4cc8a4c68caa90913c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import toml\n",
    "import os\n",
    "from subprocess import getoutput\n",
    "import textwrap\n",
    "import random\n",
    "import glob\n",
    "\n",
    "# General Project Configurations\n",
    "project_name_input = widgets.Text(value='whj3', description='Project Name:', style={'description_width': 'initial'})\n",
    "wandb_api_key_input = widgets.Text(value='855a9e4f2b1aa8b2e2e27e9c1c1373071fb23c42', description='WANDB API Key:', style={'description_width': 'initial'})\n",
    "in_json_input = widgets.Text(value='/content/LoRA/meta_lat.json', description='Input JSON Path:', style={'description_width': 'initial'})\n",
    "\n",
    "# SDXL Configurations\n",
    "gradient_checkpointing_checkbox = widgets.Checkbox(value=True, description='Gradient Checkpointing', disabled=False)\n",
    "no_half_vae_checkbox = widgets.Checkbox(value=True, description='No Half VAE', disabled=False)\n",
    "cache_text_encoder_outputs_checkbox = widgets.Checkbox(value=True, description='Cache Text Encoder Outputs', disabled=False)\n",
    "\n",
    "# Dataset Configurations\n",
    "num_repeats_input = widgets.IntText(value=20, description='Num Repeats:', style={'description_width': 'initial'})\n",
    "resolution_slider = widgets.IntSlider(value=1024, min=512, max=1024, step=128, description='Resolution:')\n",
    "keep_tokens_input = widgets.IntText(value=0, description='Keep Tokens:', style={'description_width': 'initial'})\n",
    "\n",
    "# Noise Control Configurations\n",
    "noise_control_type_dropdown = widgets.Dropdown(\n",
    "    options=['none', 'noise_offset', 'multires_noise'],\n",
    "    value='multires_noise',\n",
    "    description='Noise Control Type:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "noise_offset_num_input = widgets.FloatText(value=0.1, description='Noise Offset Num:', style={'description_width': 'initial'})\n",
    "adaptive_noise_scale_input = widgets.FloatText(value=0.01, description='Adaptive Noise Scale:', style={'description_width': 'initial'})\n",
    "\n",
    "# Training Specific Configurations\n",
    "min_timestep_input = widgets.IntText(value=100, description='Min Timestep:', style={'description_width': 'initial'})\n",
    "max_timestep_input = widgets.IntText(value=1000, description='Max Timestep:', style={'description_width': 'initial'})\n",
    "train_batch_size_input = widgets.IntText(value=20, description='Train Batch Size:', style={'description_width': 'initial'})\n",
    "mixed_precision_dropdown = widgets.Dropdown(\n",
    "    options=[\"no\", \"fp16\", \"bf16\"],\n",
    "    value='bf16',\n",
    "    description='Mixed Precision:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "num_epochs_input = widgets.IntText(value=5, description='Num Epochs:', style={'description_width': 'initial'})\n",
    "\n",
    "# Save Configurations\n",
    "save_precision_dropdown = widgets.Dropdown(\n",
    "    options=[\"float\", \"fp16\", \"bf16\"],\n",
    "    value='bf16',\n",
    "    description='Save Precision:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "save_every_n_epochs_input = widgets.IntText(value=1, description='Save Every N Epochs:', style={'description_width': 'initial'})\n",
    "\n",
    "# Sample Prompt Configurations\n",
    "enable_sample_checkbox = widgets.Checkbox(value=True, description='Enable Sample', disabled=False)\n",
    "sampler_dropdown = widgets.Dropdown(\n",
    "    options=[\"ddim\", \"pndm\", \"lms\", \"euler\", \"euler_a\", \"heun\", \"dpm_2\", \"dpm_2_a\", \"dpmsolver\", \"dpmsolver++\", \"dpmsingle\", \"k_lms\", \"k_euler\", \"k_euler_a\", \"k_dpm_2\", \"k_dpm_2_a\"],\n",
    "    value='euler_a',\n",
    "    description='Sampler:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Button to generate configuration\n",
    "generate_config_button = widgets.Button(description=\"Generate Training Configuration\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Display the widgets\n",
    "accordion = widgets.Accordion(children=[\n",
    "    widgets.VBox([project_name_input, wandb_api_key_input, in_json_input]),\n",
    "    widgets.VBox([gradient_checkpointing_checkbox, no_half_vae_checkbox, cache_text_encoder_outputs_checkbox]),\n",
    "    widgets.VBox([num_repeats_input, resolution_slider, keep_tokens_input]),\n",
    "    widgets.VBox([noise_control_type_dropdown, noise_offset_num_input, adaptive_noise_scale_input]),\n",
    "    widgets.VBox([min_timestep_input, max_timestep_input, train_batch_size_input, mixed_precision_dropdown, num_epochs_input]),\n",
    "    widgets.VBox([save_precision_dropdown, save_every_n_epochs_input, enable_sample_checkbox, sampler_dropdown])\n",
    "])\n",
    "accordion.set_title(0, 'Project Configurations')\n",
    "accordion.set_title(1, 'SDXL Configurations')\n",
    "accordion.set_title(2, 'Dataset Configurations')\n",
    "accordion.set_title(3, 'Noise Control Configurations')\n",
    "accordion.set_title(4, 'Training Specific Configurations')\n",
    "accordion.set_title(5, 'Save & Sample Configurations')\n",
    "\n",
    "display(accordion, generate_config_button, output_area)\n",
    "\n",
    "def generate_config(b):\n",
    "    # Prepare configuration dictionary based on the inputs\n",
    "    training_config = {\n",
    "        'project_name': project_name_input.value,\n",
    "        'wandb_api_key': wandb_api_key_input.value,\n",
    "        'input_json': in_json_input.value,\n",
    "        'gradient_checkpointing': gradient_checkpointing_checkbox.value,\n",
    "        'no_half_vae': no_half_vae_checkbox.value,\n",
    "        'cache_text_encoder_outputs': cache_text_encoder_outputs_checkbox.value,\n",
    "        'num_repeats': num_repeats_input.value,\n",
    "        'resolution': resolution_slider.value,\n",
    "        'keep_tokens': keep_tokens_input.value,\n",
    "        'noise_control_type': noise_control_type_dropdown.value,\n",
    "        'noise_offset_num': noise_offset_num_input.value,\n",
    "        'adaptive_noise_scale': adaptive_noise_scale_input.value,\n",
    "        'min_timestep': min_timestep_input.value,\n",
    "        'max_timestep': max_timestep_input.value,\n",
    "        'train_batch_size': train_batch_size_input.value,\n",
    "        'mixed_precision': mixed_precision_dropdown.value,\n",
    "        'num_epochs': num_epochs_input.value,\n",
    "        'save_precision': save_precision_dropdown.value,\n",
    "        'save_every_n_epochs': save_every_n_epochs_input.value,\n",
    "        'enable_sample': enable_sample_checkbox.value,\n",
    "        'sampler': sampler_dropdown.value\n",
    "    }\n",
    "\n",
    "    config_toml = toml.dumps(training_config)\n",
    "    \n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        print(\"Training Configuration:\\n\")\n",
    "        print(config_toml)\n",
    "\n",
    "generate_config_button.on_click(generate_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1856071,
     "status": "ok",
     "timestamp": 1714776576669,
     "user": {
      "displayName": "Sung Kim",
      "userId": "16507898750669832188"
     },
     "user_tz": -540
    },
    "id": "nyKHn34o2Tya",
    "outputId": "5d65fa97-08bb-4339-bd5c-6bbac7ddc29b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading settings from /content/LoRA/config/config_file.toml...\n",
      "/content/LoRA/config/config_file\n",
      "prepare tokenizers\n",
      "update token length: 225\n",
      "Training with captions.\n",
      "loading existing metadata: /content/LoRA/meta_lat.json\n",
      "metadata has bucket info, enable bucketing / メタデータにbucket情報があるためbucketを有効にします\n",
      "using bucket info in metadata / メタデータ内のbucket情報を使います\n",
      "[Dataset 0]\n",
      "  batch_size: 20\n",
      "  resolution: (1024, 1024)\n",
      "  enable_bucket: True\n",
      "  min_bucket_reso: None\n",
      "  max_bucket_reso: None\n",
      "  bucket_reso_steps: None\n",
      "  bucket_no_upscale: None\n",
      "\n",
      "  [Subset 0 of Dataset 0]\n",
      "    image_dir: \"/content/drive/MyDrive/train/WHJ4\"\n",
      "    image_count: 66\n",
      "    num_repeats: 20\n",
      "    shuffle_caption: False\n",
      "    keep_tokens: 0\n",
      "    caption_dropout_rate: 0.0\n",
      "    caption_dropout_every_n_epoches: 0\n",
      "    caption_tag_dropout_rate: 0.0\n",
      "    color_aug: False\n",
      "    flip_aug: False\n",
      "    face_crop_aug_range: None\n",
      "    random_crop: False\n",
      "    token_warmup_min: 1,\n",
      "    token_warmup_step: 0,\n",
      "    metadata_file: /content/LoRA/meta_lat.json\n",
      "\n",
      "\n",
      "[Dataset 0]\n",
      "loading image sizes.\n",
      "100% 66/66 [00:00<00:00, 985139.02it/s]\n",
      "make buckets\n",
      "number of images (including repeats) / 各bucketの画像枚数（繰り返し回数を含む）\n",
      "bucket 0: resolution (704, 1024), count: 20\n",
      "bucket 1: resolution (768, 1024), count: 400\n",
      "bucket 2: resolution (832, 1024), count: 40\n",
      "bucket 3: resolution (896, 1024), count: 20\n",
      "bucket 4: resolution (1024, 576), count: 40\n",
      "bucket 5: resolution (1024, 704), count: 40\n",
      "bucket 6: resolution (1024, 768), count: 40\n",
      "bucket 7: resolution (1024, 832), count: 40\n",
      "bucket 8: resolution (1024, 896), count: 40\n",
      "bucket 9: resolution (1024, 960), count: 20\n",
      "bucket 10: resolution (1024, 1024), count: 620\n",
      "mean ar error (without repeats): 0.0\n",
      "Warning: SDXL has been trained with noise_offset=0.0357, but noise_offset is disabled due to multires_noise_iterations / SDXLはnoise_offset=0.0357で学習されていますが、multires_noise_iterationsが有効になっているためnoise_offsetは無効になります\n",
      "preparing accelerator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstamp-gambier0x\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "loading model for process 0/1\n",
      "load Diffusers pretrained models: stablediffusionapi/newrealityxl-global-nsfw, variant=None\n",
      "The config attributes {'force_upcast': True} were passed to AutoencoderKL, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "The config attributes {'attention_type': 'default', 'dropout': 0.0} were passed to UNet2DConditionModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "U-Net converted to original U-Net\n",
      "load VAE: /content/vae/diffusion_pytorch_model.bin\n",
      "additional VAE loaded\n",
      "Enable xformers for U-Net\n",
      "import network module: networks.lora\n",
      "move vae and unet to cpu to save memory\n",
      "[Dataset 0]\n",
      "caching text encoder outputs.\n",
      "checking cache existence...\n",
      "100% 66/66 [00:00<00:00, 1287553.79it/s]\n",
      "caching text encoder outputs...\n",
      "100% 4/4 [00:00<00:00,  4.53it/s]\n",
      "move vae and unet back to original device\n",
      "create LoRA network. base dim (rank): 64, alpha: 32\n",
      "neuron dropout: p=None, rank dropout: p=None, module dropout: p=None\n",
      "create LoRA for Text Encoder 1:\n",
      "create LoRA for Text Encoder 2:\n",
      "create LoRA for Text Encoder: 264 modules.\n",
      "create LoRA for U-Net: 722 modules.\n",
      "enable LoRA for U-Net\n",
      "prepare optimizer, data loader etc.\n",
      "use Adafactor optimizer | {'scale_parameter': False, 'relative_step': False, 'warmup_init': False}\n",
      "because max_grad_norm is set, clip_grad_norm is enabled. consider set to 0 / max_grad_normが設定されているためclip_grad_normが有効になります。0に設定して無効にしたほうがいいかもしれません\n",
      "constant_with_warmup will be good / スケジューラはconstant_with_warmupが良いかもしれません\n",
      "override steps. steps for 5 epochs is / 指定エポックまでのステップ数: 330\n",
      "resume training from local state: /content/drive/MyDrive/kohya-trainer/output/whj2/whj2-state\n",
      "running training / 学習開始\n",
      "  num train images * repeats / 学習画像の数×繰り返し回数: 1320\n",
      "  num reg images / 正則化画像の数: 0\n",
      "  num batches per epoch / 1epochのバッチ数: 66\n",
      "  num epochs / epoch数: 5\n",
      "  batch size per device / バッチサイズ: 20\n",
      "  gradient accumulation steps / 勾配を合計するステップ数 = 1\n",
      "  total optimization steps / 学習ステップ数: 330\n",
      "steps:   0% 0/330 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LoRA/logs/20240503221855/wandb/run-20240503_222013-booy8wtj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcerulean-durian-4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/stamp-gambier0x/whj3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/stamp-gambier0x/whj3/runs/booy8wtj\u001b[0m\n",
      "\n",
      "epoch 1/5\n",
      "steps:  20% 66/330 [05:18<21:12,  4.82s/it, loss=0.127]\n",
      "saving checkpoint: /content/drive/MyDrive/kohya-trainer/output/whj3/whj3-000001.safetensors\n",
      "\n",
      "saving state at epoch 1\n",
      "\n",
      "generating sample images at step / サンプル画像生成 ステップ: 66\n",
      "prompt: whj, high resolution photo of 28 years old asian boy, single eyelids, youthful, face photo\n",
      "negative_prompt: double eyelids, bad quality, blur, lowres,  blurry, signature, watermark, username, artist name text, glitch, pussy, noise, noisy, black and white, drawn, painted, illustration, manga, comics, cartoon \n",
      "height: 1024\n",
      "width: 1024\n",
      "sample_steps: 28\n",
      "scale: 12\n",
      "\n",
      "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  4% 1/28 [00:00<00:12,  2.25it/s]\u001b[A\n",
      "  7% 2/28 [00:00<00:08,  2.96it/s]\u001b[A\n",
      " 11% 3/28 [00:00<00:07,  3.30it/s]\u001b[A\n",
      " 14% 4/28 [00:01<00:06,  3.49it/s]\u001b[A\n",
      " 18% 5/28 [00:01<00:06,  3.60it/s]\u001b[A\n",
      " 21% 6/28 [00:01<00:06,  3.65it/s]\u001b[A\n",
      " 25% 7/28 [00:02<00:05,  3.70it/s]\u001b[A\n",
      " 29% 8/28 [00:02<00:05,  3.75it/s]\u001b[A\n",
      " 32% 9/28 [00:02<00:05,  3.53it/s]\u001b[A\n",
      " 36% 10/28 [00:02<00:05,  3.53it/s]\u001b[A\n",
      " 39% 11/28 [00:03<00:04,  3.50it/s]\u001b[A\n",
      " 43% 12/28 [00:03<00:04,  3.46it/s]\u001b[A\n",
      " 46% 13/28 [00:03<00:04,  3.47it/s]\u001b[A\n",
      " 50% 14/28 [00:04<00:04,  3.41it/s]\u001b[A\n",
      " 54% 15/28 [00:04<00:04,  3.24it/s]\u001b[A\n",
      " 57% 16/28 [00:04<00:03,  3.13it/s]\u001b[A\n",
      " 61% 17/28 [00:05<00:03,  3.06it/s]\u001b[A\n",
      " 64% 18/28 [00:05<00:03,  3.03it/s]\u001b[A\n",
      " 68% 19/28 [00:05<00:02,  3.08it/s]\u001b[A\n",
      " 71% 20/28 [00:06<00:02,  2.98it/s]\u001b[A\n",
      " 75% 21/28 [00:06<00:02,  2.94it/s]\u001b[A\n",
      " 79% 22/28 [00:06<00:01,  3.00it/s]\u001b[A\n",
      " 82% 23/28 [00:07<00:01,  2.96it/s]\u001b[A\n",
      " 86% 24/28 [00:07<00:01,  3.04it/s]\u001b[A\n",
      " 89% 25/28 [00:07<00:00,  3.11it/s]\u001b[A\n",
      " 93% 26/28 [00:08<00:00,  3.08it/s]\u001b[A\n",
      " 96% 27/28 [00:08<00:00,  3.08it/s]\u001b[A\n",
      "100% 28/28 [00:08<00:00,  3.05it/s]\u001b[A\n",
      "                                   \u001b[A\n",
      "epoch 2/5\n",
      "steps:  40% 132/330 [11:07<16:41,  5.06s/it, loss=0.127]\n",
      "saving checkpoint: /content/drive/MyDrive/kohya-trainer/output/whj3/whj3-000002.safetensors\n",
      "\n",
      "saving state at epoch 2\n",
      "removing old state: /content/drive/MyDrive/kohya-trainer/output/whj3/whj3-000001-state\n",
      "\n",
      "generating sample images at step / サンプル画像生成 ステップ: 132\n",
      "prompt: whj, high resolution photo of 28 years old asian boy, single eyelids, youthful, face photo\n",
      "negative_prompt: double eyelids, bad quality, blur, lowres,  blurry, signature, watermark, username, artist name text, glitch, pussy, noise, noisy, black and white, drawn, painted, illustration, manga, comics, cartoon \n",
      "height: 1024\n",
      "width: 1024\n",
      "sample_steps: 28\n",
      "scale: 12\n",
      "\n",
      "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  4% 1/28 [00:00<00:07,  3.50it/s]\u001b[A\n",
      "  7% 2/28 [00:00<00:07,  3.64it/s]\u001b[A\n",
      " 11% 3/28 [00:00<00:06,  3.72it/s]\u001b[A\n",
      " 14% 4/28 [00:01<00:06,  3.76it/s]\u001b[A\n",
      " 18% 5/28 [00:01<00:06,  3.77it/s]\u001b[A\n",
      " 21% 6/28 [00:01<00:05,  3.79it/s]\u001b[A\n",
      " 25% 7/28 [00:01<00:05,  3.79it/s]\u001b[A\n",
      " 29% 8/28 [00:02<00:05,  3.80it/s]\u001b[A\n",
      " 32% 9/28 [00:02<00:05,  3.70it/s]\u001b[A\n",
      " 36% 10/28 [00:02<00:04,  3.69it/s]\u001b[A\n",
      " 39% 11/28 [00:02<00:04,  3.73it/s]\u001b[A\n",
      " 43% 12/28 [00:03<00:04,  3.76it/s]\u001b[A\n",
      " 46% 13/28 [00:03<00:04,  3.67it/s]\u001b[A\n",
      " 50% 14/28 [00:03<00:03,  3.63it/s]\u001b[A\n",
      " 54% 15/28 [00:04<00:03,  3.68it/s]\u001b[A\n",
      " 57% 16/28 [00:04<00:03,  3.64it/s]\u001b[A\n",
      " 61% 17/28 [00:04<00:03,  3.56it/s]\u001b[A\n",
      " 64% 18/28 [00:04<00:02,  3.60it/s]\u001b[A\n",
      " 68% 19/28 [00:05<00:02,  3.65it/s]\u001b[A\n",
      " 71% 20/28 [00:05<00:02,  3.69it/s]\u001b[A\n",
      " 75% 21/28 [00:05<00:01,  3.74it/s]\u001b[A\n",
      " 79% 22/28 [00:05<00:01,  3.77it/s]\u001b[A\n",
      " 82% 23/28 [00:06<00:01,  3.79it/s]\u001b[A\n",
      " 86% 24/28 [00:06<00:01,  3.79it/s]\u001b[A\n",
      " 89% 25/28 [00:06<00:00,  3.79it/s]\u001b[A\n",
      " 93% 26/28 [00:06<00:00,  3.80it/s]\u001b[A\n",
      " 96% 27/28 [00:07<00:00,  3.80it/s]\u001b[A\n",
      "100% 28/28 [00:07<00:00,  3.78it/s]\u001b[A\n",
      "                                   \u001b[A\n",
      "epoch 3/5\n",
      "steps:  60% 198/330 [16:56<11:17,  5.13s/it, loss=0.129]\n",
      "saving checkpoint: /content/drive/MyDrive/kohya-trainer/output/whj3/whj3-000003.safetensors\n",
      "\n",
      "saving state at epoch 3\n",
      "removing old state: /content/drive/MyDrive/kohya-trainer/output/whj3/whj3-000002-state\n",
      "\n",
      "generating sample images at step / サンプル画像生成 ステップ: 198\n",
      "prompt: whj, high resolution photo of 28 years old asian boy, single eyelids, youthful, face photo\n",
      "negative_prompt: double eyelids, bad quality, blur, lowres,  blurry, signature, watermark, username, artist name text, glitch, pussy, noise, noisy, black and white, drawn, painted, illustration, manga, comics, cartoon \n",
      "height: 1024\n",
      "width: 1024\n",
      "sample_steps: 28\n",
      "scale: 12\n",
      "\n",
      "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  4% 1/28 [00:00<00:08,  3.36it/s]\u001b[A\n",
      "  7% 2/28 [00:00<00:07,  3.42it/s]\u001b[A\n",
      " 11% 3/28 [00:00<00:07,  3.51it/s]\u001b[A\n",
      " 14% 4/28 [00:01<00:06,  3.62it/s]\u001b[A\n",
      " 18% 5/28 [00:01<00:06,  3.59it/s]\u001b[A\n",
      " 21% 6/28 [00:01<00:06,  3.58it/s]\u001b[A\n",
      " 25% 7/28 [00:01<00:05,  3.65it/s]\u001b[A\n",
      " 29% 8/28 [00:02<00:05,  3.69it/s]\u001b[A\n",
      " 32% 9/28 [00:02<00:05,  3.72it/s]\u001b[A\n",
      " 36% 10/28 [00:02<00:04,  3.74it/s]\u001b[A\n",
      " 39% 11/28 [00:03<00:04,  3.75it/s]\u001b[A\n",
      " 43% 12/28 [00:03<00:04,  3.73it/s]\u001b[A\n",
      " 46% 13/28 [00:03<00:04,  3.70it/s]\u001b[A\n",
      " 50% 14/28 [00:03<00:03,  3.63it/s]\u001b[A\n",
      " 54% 15/28 [00:04<00:03,  3.59it/s]\u001b[A\n",
      " 57% 16/28 [00:04<00:03,  3.58it/s]\u001b[A\n",
      " 61% 17/28 [00:04<00:03,  3.54it/s]\u001b[A\n",
      " 64% 18/28 [00:04<00:02,  3.56it/s]\u001b[A\n",
      " 68% 19/28 [00:05<00:02,  3.57it/s]\u001b[A\n",
      " 71% 20/28 [00:05<00:02,  3.56it/s]\u001b[A\n",
      " 75% 21/28 [00:05<00:01,  3.52it/s]\u001b[A\n",
      " 79% 22/28 [00:06<00:01,  3.46it/s]\u001b[A\n",
      " 82% 23/28 [00:06<00:01,  3.39it/s]\u001b[A\n",
      " 86% 24/28 [00:06<00:01,  3.37it/s]\u001b[A\n",
      " 89% 25/28 [00:07<00:00,  3.36it/s]\u001b[A\n",
      " 93% 26/28 [00:07<00:00,  3.39it/s]\u001b[A\n",
      " 96% 27/28 [00:07<00:00,  3.36it/s]\u001b[A\n",
      "100% 28/28 [00:07<00:00,  3.42it/s]\u001b[A\n",
      "                                   \u001b[A\n",
      "epoch 4/5\n",
      "steps:  80% 264/330 [22:43<05:40,  5.16s/it, loss=0.133]\n",
      "saving checkpoint: /content/drive/MyDrive/kohya-trainer/output/whj3/whj3-000004.safetensors\n",
      "\n",
      "saving state at epoch 4\n",
      "removing old state: /content/drive/MyDrive/kohya-trainer/output/whj3/whj3-000003-state\n",
      "\n",
      "generating sample images at step / サンプル画像生成 ステップ: 264\n",
      "prompt: whj, high resolution photo of 28 years old asian boy, single eyelids, youthful, face photo\n",
      "negative_prompt: double eyelids, bad quality, blur, lowres,  blurry, signature, watermark, username, artist name text, glitch, pussy, noise, noisy, black and white, drawn, painted, illustration, manga, comics, cartoon \n",
      "height: 1024\n",
      "width: 1024\n",
      "sample_steps: 28\n",
      "scale: 12\n",
      "\n",
      "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  4% 1/28 [00:00<00:07,  3.47it/s]\u001b[A\n",
      "  7% 2/28 [00:00<00:07,  3.64it/s]\u001b[A\n",
      " 11% 3/28 [00:00<00:06,  3.70it/s]\u001b[A\n",
      " 14% 4/28 [00:01<00:06,  3.72it/s]\u001b[A\n",
      " 18% 5/28 [00:01<00:06,  3.75it/s]\u001b[A\n",
      " 21% 6/28 [00:01<00:05,  3.75it/s]\u001b[A\n",
      " 25% 7/28 [00:01<00:05,  3.75it/s]\u001b[A\n",
      " 29% 8/28 [00:02<00:05,  3.77it/s]\u001b[A\n",
      " 32% 9/28 [00:02<00:05,  3.79it/s]\u001b[A\n",
      " 36% 10/28 [00:02<00:04,  3.78it/s]\u001b[A\n",
      " 39% 11/28 [00:02<00:04,  3.80it/s]\u001b[A\n",
      " 43% 12/28 [00:03<00:04,  3.78it/s]\u001b[A\n",
      " 46% 13/28 [00:03<00:03,  3.79it/s]\u001b[A\n",
      " 50% 14/28 [00:03<00:03,  3.74it/s]\u001b[A\n",
      " 54% 15/28 [00:04<00:03,  3.68it/s]\u001b[A\n",
      " 57% 16/28 [00:04<00:03,  3.67it/s]\u001b[A\n",
      " 61% 17/28 [00:04<00:03,  3.65it/s]\u001b[A\n",
      " 64% 18/28 [00:04<00:02,  3.66it/s]\u001b[A\n",
      " 68% 19/28 [00:05<00:02,  3.68it/s]\u001b[A\n",
      " 71% 20/28 [00:05<00:02,  3.69it/s]\u001b[A\n",
      " 75% 21/28 [00:05<00:01,  3.64it/s]\u001b[A\n",
      " 79% 22/28 [00:05<00:01,  3.60it/s]\u001b[A\n",
      " 82% 23/28 [00:06<00:01,  3.58it/s]\u001b[A\n",
      " 86% 24/28 [00:06<00:01,  3.61it/s]\u001b[A\n",
      " 89% 25/28 [00:06<00:00,  3.60it/s]\u001b[A\n",
      " 93% 26/28 [00:07<00:00,  3.61it/s]\u001b[A\n",
      " 96% 27/28 [00:07<00:00,  3.61it/s]\u001b[A\n",
      "100% 28/28 [00:07<00:00,  3.54it/s]\u001b[A\n",
      "                                   \u001b[A\n",
      "epoch 5/5\n",
      "steps: 100% 330/330 [28:33<00:00,  5.19s/it, loss=0.125]\n",
      "generating sample images at step / サンプル画像生成 ステップ: 330\n",
      "prompt: whj, high resolution photo of 28 years old asian boy, single eyelids, youthful, face photo\n",
      "negative_prompt: double eyelids, bad quality, blur, lowres,  blurry, signature, watermark, username, artist name text, glitch, pussy, noise, noisy, black and white, drawn, painted, illustration, manga, comics, cartoon \n",
      "height: 1024\n",
      "width: 1024\n",
      "sample_steps: 28\n",
      "scale: 12\n",
      "\n",
      "  0% 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  4% 1/28 [00:00<00:07,  3.52it/s]\u001b[A\n",
      "  7% 2/28 [00:00<00:07,  3.71it/s]\u001b[A\n",
      " 11% 3/28 [00:00<00:06,  3.74it/s]\u001b[A\n",
      " 14% 4/28 [00:01<00:06,  3.76it/s]\u001b[A\n",
      " 18% 5/28 [00:01<00:06,  3.77it/s]\u001b[A\n",
      " 21% 6/28 [00:01<00:05,  3.77it/s]\u001b[A\n",
      " 25% 7/28 [00:01<00:05,  3.79it/s]\u001b[A\n",
      " 29% 8/28 [00:02<00:05,  3.80it/s]\u001b[A\n",
      " 32% 9/28 [00:02<00:04,  3.80it/s]\u001b[A\n",
      " 36% 10/28 [00:02<00:04,  3.83it/s]\u001b[A\n",
      " 39% 11/28 [00:02<00:04,  3.85it/s]\u001b[A\n",
      " 43% 12/28 [00:03<00:04,  3.86it/s]\u001b[A\n",
      " 46% 13/28 [00:03<00:03,  3.85it/s]\u001b[A\n",
      " 50% 14/28 [00:03<00:03,  3.84it/s]\u001b[A\n",
      " 54% 15/28 [00:03<00:03,  3.83it/s]\u001b[A\n",
      " 57% 16/28 [00:04<00:03,  3.82it/s]\u001b[A\n",
      " 61% 17/28 [00:04<00:02,  3.79it/s]\u001b[A\n",
      " 64% 18/28 [00:04<00:02,  3.80it/s]\u001b[A\n",
      " 68% 19/28 [00:05<00:02,  3.81it/s]\u001b[A\n",
      " 71% 20/28 [00:05<00:02,  3.82it/s]\u001b[A\n",
      " 75% 21/28 [00:05<00:01,  3.81it/s]\u001b[A\n",
      " 79% 22/28 [00:05<00:01,  3.80it/s]\u001b[A\n",
      " 82% 23/28 [00:06<00:01,  3.81it/s]\u001b[A\n",
      " 86% 24/28 [00:06<00:01,  3.82it/s]\u001b[A\n",
      " 89% 25/28 [00:06<00:00,  3.79it/s]\u001b[A\n",
      " 93% 26/28 [00:06<00:00,  3.79it/s]\u001b[A\n",
      " 96% 27/28 [00:07<00:00,  3.79it/s]\u001b[A\n",
      "100% 28/28 [00:07<00:00,  3.79it/s]\u001b[A\n",
      "                                   \u001b[A\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss/average █▄▄▂▂▂▂▂▂▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▃▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss/current ▇▃▃▃▃▆▇▅▂▃▅▄▆▂█▃▆▃▆▃▆▅▄▂▅▄▄▃▆▅▃▅▃▂▃▂▃▁▂▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      lr/unet ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss/average 0.12531\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: loss/current 0.12587\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      lr/unet 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcerulean-durian-4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/stamp-gambier0x/whj3/runs/booy8wtj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ️⚡ View job at \u001b[34m\u001b[4mhttps://wandb.ai/stamp-gambier0x/whj3/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2ODcyODA1MA==/version_details/v2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 5 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m/content/LoRA/logs/20240503221855/wandb/run-20240503_222013-booy8wtj/logs\u001b[0m\n",
      "\n",
      "saving last state.\n",
      "\n",
      "saving checkpoint: /content/drive/MyDrive/kohya-trainer/output/whj3/whj3.safetensors\n",
      "model saved.\n",
      "steps: 100% 330/330 [29:17<00:00,  5.33s/it, loss=0.125]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#@title ## **4.5. Start Training**\n",
    "import os\n",
    "import toml\n",
    "\n",
    "#@markdown Check your config here if you want to edit something:\n",
    "#@markdown - `sample_prompt` : /content/LoRA/config/sample_prompt.toml\n",
    "#@markdown - `config_file` : /content/LoRA/config/config_file.toml\n",
    "\n",
    "\n",
    "#@markdown You can import config from another session if you want.\n",
    "\n",
    "sample_prompt   = \"/content/LoRA/config/sample_prompt.toml\" #@param {type:'string'}\n",
    "config_file     = \"/content/LoRA/config/config_file.toml\" #@param {type:'string'}\n",
    "\n",
    "def read_file(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        contents = f.read()\n",
    "    return contents\n",
    "\n",
    "def train(config):\n",
    "    args = \"\"\n",
    "    for k, v in config.items():\n",
    "        if k.startswith(\"_\"):\n",
    "            args += f'\"{v}\" '\n",
    "        elif isinstance(v, str):\n",
    "            args += f'--{k}=\"{v}\" '\n",
    "        elif isinstance(v, bool) and v:\n",
    "            args += f\"--{k} \"\n",
    "        elif isinstance(v, float) and not isinstance(v, bool):\n",
    "            args += f\"--{k}={v} \"\n",
    "        elif isinstance(v, int) and not isinstance(v, bool):\n",
    "            args += f\"--{k}={v} \"\n",
    "\n",
    "    return args\n",
    "\n",
    "accelerate_conf = {\n",
    "    \"config_file\" : \"/content/kohya-trainer/accelerate_config/config.yaml\",\n",
    "    \"num_cpu_threads_per_process\" : 1,\n",
    "}\n",
    "\n",
    "train_conf = {\n",
    "    \"sample_prompts\"  : sample_prompt if os.path.exists(sample_prompt) else None,\n",
    "    \"config_file\"     : config_file,\n",
    "    \"wandb_api_key\"   : wandb_api_key if wandb_api_key else None\n",
    "}\n",
    "\n",
    "accelerate_args = train(accelerate_conf)\n",
    "train_args = train(train_conf)\n",
    "\n",
    "final_args = f\"accelerate launch {accelerate_args} sdxl_train_network.py {train_args}\"\n",
    "\n",
    "os.chdir(repo_dir)\n",
    "!{final_args}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "tTVqCAgSmie4"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1PuZheFpvcP-bilBpKOktWfPE4sDCxvGd",
     "timestamp": 1714782131020
    },
    {
     "file_id": "1zsoMW1c3bNjui4vNeUD7vP-3YY_Qj6yJ",
     "timestamp": 1714779301801
    },
    {
     "file_id": "1s9dS6r8QH5eOWgszgfD2zqmABUp2pZ3b",
     "timestamp": 1714463021197
    },
    {
     "file_id": "1RuDeNfQYlTESeUpQcIvzRRhXUp8m_cL6",
     "timestamp": 1713956207939
    },
    {
     "file_id": "1NQjicmp6QU0j_dSPKk5CagIWsMcEGcFm",
     "timestamp": 1713413135755
    },
    {
     "file_id": "1y7B2fq3tVxnBdGtQG5JpuCFSLx8PTnN0",
     "timestamp": 1712112028750
    },
    {
     "file_id": "1tyjZOM3GeJsldiN0xleR5boP4Ycpamgs",
     "timestamp": 1711647167751
    },
    {
     "file_id": "1rtPXLDWb--QOaGT9PoGvEx87bsSiQYsH",
     "timestamp": 1709772036664
    },
    {
     "file_id": "19dsj1vRJ87tq7ILAuhHU67Y-0EIMLU_l",
     "timestamp": 1709381746479
    },
    {
     "file_id": "1SIokMXL8m42zb1tSiW4eTLfefNgJauvj",
     "timestamp": 1707334705049
    },
    {
     "file_id": "15pWO0x55zbMO-mAae54CQDhnL4Y062Xg",
     "timestamp": 1706706406114
    },
    {
     "file_id": "https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-trainer-XL.ipynb",
     "timestamp": 1706472829207
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
